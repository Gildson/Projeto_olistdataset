# -*- coding: utf-8 -*-
"""Rede_Neural_NARX_previsão_de_compra_e-commerce.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yNQkkUMpZWz6tWz9Icnz23Gjiqso2PwP
"""

# Commented out IPython magic to ensure Python compatibility.
#Importações das bibliotecas que utilizaremos no projeto
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
plt.style.use('classic')
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

#Leitura do arquivo csv
dataSP2017 = pd.read_csv('/content/drive/MyDrive/olist_data_2017.csv')

dataSP2017 = dataSP2017.drop(columns = ['Unnamed: 0', 'payment_value', 'order_purchase_timestamp'])

dataSP2017

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from matplotlib import pyplot as plt

X = dataSP2017.iloc[:, 0 : dataSP2017.shape[1] - 4].values
y = dataSP2017.iloc[:, 4]
labelencoder = LabelEncoder()
X[:,2] = labelencoder.fit_transform(X[:,2])

X = np.asarray(X).astype(np.float32)
Y = np.asarray(y).astype(np.float32)

#Separação dos dados, utilizaremos 70% para treinamento e 30% para teste
X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, Y, test_size = 0.3, random_state = 0)

numPreviousSteps = 5
inputShape = (None, numPreviousSteps + 2)

class Narx(keras.Model):

    def __init__(self):
        super(Narx, self).__init__(name='narx')
        self.dense = keras.layers.Dense(100, input_shape=inputShape, activation="relu")
        self.batchnormalization = keras.layers.BatchNormalization()
        self.dropout = keras.layers.Dropout(0.2)
        self.dense = keras.layers.Dense(100, activation="relu")
        self.batchnormalization = keras.layers.BatchNormalization()
        self.dropout = keras.layers.Dropout(0.2)
        self.dense = keras.layers.Dense(100, activation="relu")
        self.batchnormalization = keras.layers.BatchNormalization()
        self.dropout = keras.layers.Dropout(0.2)


        self.outputLayer = keras.layers.Dense(1, activation=keras.activations.linear)

    def call(self, inputs, training = False):
        if (training):
            x = self.dense(inputs)
            return self.outputLayer(x)
        else:
            x = self.dense(inputs)
            return self.outputLayer(x)

#Criação do modelo de rede neural para uma regressão
model = Narx()
opt = tf.keras.optimizers.RMSprop(0.001)
model.compile(optimizer=opt,
              loss=tf.losses.mean_absolute_error,
              metrics=['mae'])

previsoes = model.predict(X_teste)

model.summary()

history = model.fit(X, Y, epochs=100, validation_split = 0.3, verbose=0)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure(figsize=(10,7))
  plt.xlabel('Épocas')
  plt.ylabel('Erro')
  plt.title("Curva do erro médio absoluto")
  plt.plot(hist['epoch'], hist['mae'], color="red", 
           label='Erro de treinamento')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Erro de validação')
  plt.ylim([1,5])
  plt.legend()
  plt.savefig('erro.png', format='png')
  plt.show()

plot_history(history)